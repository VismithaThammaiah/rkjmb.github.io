<body>
<div style="white-space:pre-wrap;">
import pandas as pd
from pandas import DataFrame
df_tennis=DataFrame.from_csv(r"C:\\Users\\Student\\Desktop\\PlayTennis.csv")

def entropy(probs):
    import math
    return sum([-prob*math.log(prob,2) for prob in probs])

def entropyOfList(a_list):
    
    from collections import Counter
    cnt=Counter(x for x in a_list)

    print("no or yes cases", a_list.name,cnt)
    num_instances=len(a_list)*1.0
    probs=[x/num_instances for x in cnt.values()]
    return entropy(probs)
total_entropy=entropyOfList(df_tennis["PlayTennis"])
print("entropy of given dataset:",total_entropy)

def information_gain(df, split_attribute_name, target_attribute_name, trace=0):
    print("Information Gain Calculation of ",split_attribute_name)
    df_split = df.groupby(split_attribute_name)
    nobs = len(df.index) * 1.0
    print('nobs',nobs)
    df_agg_ent = df_split.agg({target_attribute_name : [entropyOfList, lambda x: len(x)/nobs] })[target_attribute_name]
    
    df_agg_ent.columns = ['Entropy', 'PropObservations']
    new_entropy = sum( df_agg_ent['Entropy'] * df_agg_ent['PropObservations'] )
    old_entropy = entropyOfList(df[target_attribute_name])
    return old_entropy - new_entropy
print('Info-gain for Outlook is :'+str(information_gain(df_tennis, 'Outlook', 'PlayTennis')),"\n")
print('\n Info-gain for Humidity is: ' + str(information_gain(df_tennis, 'Humidity', 'PlayTennis')),"\n")
print('\n Info-gain for Wind is:' + str(information_gain(df_tennis, 'Wind', 'PlayTennis')),"\n")
print('\n Info-gain for Temperature is:' + str(information_gain(df_tennis, 'Temperature','PlayTennis')),"\n")

def id3(df, target_attribute_name, attribute_names, default_class=None):
   
    from collections import Counter
    cnt = Counter(x for x in df[target_attribute_name])# class of YES /NO
        
    if len(cnt) == 1:
        return next(iter(cnt))  
    
    elif df.empty or (not attribute_names):
        return default_class   
    
    else:        
        default_class = max(cnt.keys())         
        gainz = [information_gain(df, attr, target_attribute_name) for attr in attribute_names] #
        index_of_max = gainz.index(max(gainz)) 
        best_attr = attribute_names[index_of_max]
        
        
        tree = {best_attr:{}} 
        remaining_attribute_names = [i for i in attribute_names if i != best_attr]
        
       
        for attr_val, data_subset in df.groupby(best_attr):
            subtree = id3(data_subset,
                        target_attribute_name,
                        remaining_attribute_names,
                        default_class)
            tree[best_attr][attr_val] = subtree
        return tree

attribute_names = list(df_tennis.columns)
print("List of Attributes:", attribute_names) 
attribute_names.remove('PlayTennis') 
print("Predicting Attributes:", attribute_names)


from pprint import pprint
tree = id3(df_tennis,'PlayTennis',attribute_names)
print("\n\nThe Resultant Decision Tree is :\n")

pprint(tree)
attribute = next(iter(tree))
print("Best Attribute :\n",attribute)
print("Tree Keys:\n",tree[attribute].keys())
</div>
</body>
